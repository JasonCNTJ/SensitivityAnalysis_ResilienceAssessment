{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入数据\n",
    "params = np.loadtxt('0823paramsnp.txt')\n",
    "edpResults = np.loadtxt('0823edpResult.txt')\n",
    "# edpResults[:, [3, 7]] = edpResults[:, [7, 3]]\n",
    "# 转化为torch tensor\n",
    "train_x = torch.from_numpy(params[:1600]).to(torch.float)\n",
    "train_y = torch.from_numpy(edpResults[:1600, 3:7]).to(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultitaskGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(MultitaskGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.MultitaskMean(\n",
    "            gpytorch.means.ConstantMean(), num_tasks=4\n",
    "        )\n",
    "        self.covar_module = gpytorch.kernels.MultitaskKernel(\n",
    "            gpytorch.kernels.RBFKernel(), num_tasks=4, rank=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=4)\n",
    "model = MultitaskGPModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/50 - Loss: 1.115\n",
      "Iter 2/50 - Loss: 1.076\n",
      "Iter 3/50 - Loss: 1.036\n",
      "Iter 4/50 - Loss: 0.995\n",
      "Iter 5/50 - Loss: 0.954\n",
      "Iter 6/50 - Loss: 0.913\n",
      "Iter 7/50 - Loss: 0.871\n",
      "Iter 8/50 - Loss: 0.829\n",
      "Iter 9/50 - Loss: 0.786\n",
      "Iter 10/50 - Loss: 0.742\n",
      "Iter 11/50 - Loss: 0.698\n",
      "Iter 12/50 - Loss: 0.653\n",
      "Iter 13/50 - Loss: 0.607\n",
      "Iter 14/50 - Loss: 0.562\n",
      "Iter 15/50 - Loss: 0.515\n",
      "Iter 16/50 - Loss: 0.469\n",
      "Iter 17/50 - Loss: 0.422\n",
      "Iter 18/50 - Loss: 0.374\n",
      "Iter 19/50 - Loss: 0.327\n",
      "Iter 20/50 - Loss: 0.279\n",
      "Iter 21/50 - Loss: 0.231\n",
      "Iter 22/50 - Loss: 0.182\n",
      "Iter 23/50 - Loss: 0.133\n",
      "Iter 24/50 - Loss: 0.083\n",
      "Iter 25/50 - Loss: 0.034\n",
      "Iter 26/50 - Loss: -0.017\n",
      "Iter 27/50 - Loss: -0.067\n",
      "Iter 28/50 - Loss: -0.117\n",
      "Iter 29/50 - Loss: -0.168\n",
      "Iter 30/50 - Loss: -0.218\n",
      "Iter 31/50 - Loss: -0.269\n",
      "Iter 32/50 - Loss: -0.319\n",
      "Iter 33/50 - Loss: -0.369\n",
      "Iter 34/50 - Loss: -0.420\n",
      "Iter 35/50 - Loss: -0.471\n",
      "Iter 36/50 - Loss: -0.521\n",
      "Iter 37/50 - Loss: -0.572\n",
      "Iter 38/50 - Loss: -0.622\n",
      "Iter 39/50 - Loss: -0.671\n",
      "Iter 40/50 - Loss: -0.721\n",
      "Iter 41/50 - Loss: -0.771\n",
      "Iter 42/50 - Loss: -0.820\n",
      "Iter 43/50 - Loss: -0.869\n",
      "Iter 44/50 - Loss: -0.917\n",
      "Iter 45/50 - Loss: -0.965\n",
      "Iter 46/50 - Loss: -1.012\n",
      "Iter 47/50 - Loss: -1.059\n",
      "Iter 48/50 - Loss: -1.105\n",
      "Iter 49/50 - Loss: -1.150\n",
      "Iter 50/50 - Loss: -1.195\n"
     ]
    }
   ],
   "source": [
    "# this is for running the notebook in our testing framework\n",
    "import os\n",
    "smoke_test = ('CI' in os.environ)\n",
    "training_iterations = 2 if smoke_test else 50\n",
    "\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "for i in range(training_iterations):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_x)\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iterations, loss.item()))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转化为torch tensor\n",
    "test_x = torch.from_numpy(params[1600:]).to(torch.float)\n",
    "test_y = torch.from_numpy(edpResults[1600:, :3]).to(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set into eval mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    predictions_test = likelihood(model(test_x))\n",
    "    mean_test = predictions_test.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\envs\\torchenv310\\lib\\site-packages\\gpytorch\\models\\exact_gp.py:284: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Set into eval mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    predictions_train = likelihood(model(train_x))\n",
    "    mean_train = predictions_train.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集：\n",
      "idr1: 0.95659837014623\n",
      "idr2: 0.8005295535966\n",
      "idr3: 0.8781310005449208\n",
      "ridr: 0.8790380352330991\n",
      "测试集：\n",
      "idr1: 0.9684621695815921\n",
      "idr2: 0.7849087237375978\n",
      "idr3: 0.8319429789658452\n",
      "ridr: 0.8574230193771735\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "# 训练集\n",
    "print('训练集：')\n",
    "r_squared_sklearn_train1 = r2_score(edpResults[:1600, 3], mean_train[:, 0].numpy())\n",
    "print('idr1:', r_squared_sklearn_train1)\n",
    "r_squared_sklearn_train2 = r2_score(edpResults[:1600, 4], mean_train[:, 1].numpy())\n",
    "print('idr2:', r_squared_sklearn_train2)\n",
    "r_squared_sklearn_train3 = r2_score(edpResults[:1600, 5], mean_train[:, 2].numpy())\n",
    "print('idr3:', r_squared_sklearn_train3)\n",
    "r_squared_sklearn_train4 = r2_score(edpResults[:1600, 6], mean_train[:, 3].numpy())\n",
    "print('ridr:', r_squared_sklearn_train4)\n",
    "# r_squared_sklearn_train4 = r2_score(edpResults[:1600, 4], mean_train[:, 4].numpy())\n",
    "# print('ridr:', r_squared_sklearn_train4)\n",
    "# r_squared_sklearn_train4 = r2_score(edpResults[:1600, 5], mean_train[:, 5].numpy())\n",
    "# print('ridr:', r_squared_sklearn_train4)\n",
    "# r_squared_sklearn_train4 = r2_score(edpResults[:1600, 6], mean_train[:, 6].numpy())\n",
    "# print('ridr:', r_squared_sklearn_train4)\n",
    "# 测试集\n",
    "print('测试集：')\n",
    "r_squared_sklearn_test1 = r2_score(edpResults[1600:, 3], mean_test[:, 0].numpy())\n",
    "print('idr1:', r_squared_sklearn_test1)\n",
    "r_squared_sklearn_test2 = r2_score(edpResults[1600:, 4], mean_test[:, 1].numpy())\n",
    "print('idr2:', r_squared_sklearn_test2)\n",
    "r_squared_sklearn_test3 = r2_score(edpResults[1600:, 5], mean_test[:, 2].numpy())\n",
    "print('idr3:', r_squared_sklearn_test3)\n",
    "r_squared_sklearn_test4 = r2_score(edpResults[1600:, 6], mean_test[:, 3].numpy())\n",
    "print('ridr:', r_squared_sklearn_test4)\n",
    "# r_squared_sklearn_test2 = r2_score(edpResults[1600:, 4], mean_test[:, 4].numpy())\n",
    "# print('idr2:', r_squared_sklearn_test2)\n",
    "# r_squared_sklearn_test3 = r2_score(edpResults[1600:, 5], mean_test[:, 5].numpy())\n",
    "# print('idr3:', r_squared_sklearn_test3)\n",
    "# r_squared_sklearn_test4 = r2_score(edpResults[1600:, 6], mean_test[:, 6].numpy())\n",
    "# print('ridr:', r_squared_sklearn_test4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
